from pathlib import Path
import pandas as pd
import requests
from datetime import datetime, timedelta
import json
from pathlib import Path
from dotenv import load_dotenv
import numpy as np
import os

# Import from config.py
from config import (
    TARGET_VARIABLE, DATA_DIR, SE3_PRICES_FILE, SWEDEN_GRID_FILE,
    TIME_FEATURES_FILE, HOLIDAYS_FILE, WEATHER_DATA_FILE,
    GRID_FEATURES, MARKET_FEATURES, PRICE_FEATURES, TIME_FEATURES, HOLIDAY_FEATURES,
    FEATURE_GROUPS, WEATHER_FEATURES
)

# Get project root from DATA_DIR
project_root = DATA_DIR.resolve().parents[1]

def update_price_data():
    """Update the price data from the API and calculate features"""
    # Use paths from config
    csv_file_path = DATA_DIR / SE3_PRICES_FILE.name

    # Check if the file exists
    if not csv_file_path.exists():
        df = pd.DataFrame(columns=['HourSE', 'PriceArea', TARGET_VARIABLE])
        # Start from 30 days ago if creating a new file to avoid excessive API calls
        latest_timestamp = pd.Timestamp.now() - timedelta(days=30)
        print(f"Creating new price data file. Starting from {latest_timestamp}")
    else:
        # Read the existing CSV file
        df = pd.read_csv(csv_file_path)
        df['HourSE'] = pd.to_datetime(df['HourSE'])
        
        # If the file exists but is empty or corrupted, start from 30 days ago
        if df.empty:
            latest_timestamp = pd.Timestamp.now() - timedelta(days=30)
            print(f"Existing file is empty. Starting from {latest_timestamp}")
        else:
            # Get the latest timestamp from the existing data
            latest_timestamp = df['HourSE'].max()
            print(f"Continuing from latest timestamp in file: {latest_timestamp}")

    # Get the current time
    current_time = datetime.now()
    new_data = []

    # Loop through missing hours until the present
    while latest_timestamp < current_time:
        latest_timestamp += timedelta(hours=1)
        next_date_str = latest_timestamp.strftime('%Y-%m-%d')
        next_hour_int = latest_timestamp.hour
        next_hour_str = latest_timestamp.strftime('%H:00:00')

        # Fetch data from the API
        api_url = f'https://mgrey.se/espot?format=json&date={next_date_str}'
        try:
            response = requests.get(api_url, timeout=10)  # Add timeout for safety
            
            if response.status_code == 200:
                data = response.json()
                if 'SE3' in data:
                    se3_data = data['SE3']
                    hour_data = next((item for item in se3_data if item['hour'] == next_hour_int), None)
                    
                    if hour_data:
                        new_data.append({
                            'HourSE': f'{next_date_str} {next_hour_str}',
                            'PriceArea': 'SE3',
                            TARGET_VARIABLE: hour_data['price_sek'],
                        })
                    else:
                        print(f"No data available for {next_date_str} {next_hour_str}")
                else:
                    print(f"No SE3 data available for {next_date_str}")
            else:
                print(f"Failed to fetch price data for {next_date_str}. Status code: {response.status_code}")
                break
        except Exception as e:
            print(f"Error fetching price data: {e}")
            break

    # Update DataFrame with new data
    if new_data:
        new_df = pd.DataFrame(new_data)
        new_df['HourSE'] = pd.to_datetime(new_df['HourSE'])
        df = pd.concat([df, new_df], ignore_index=True)
        df = df.sort_values(by="HourSE")

        # Calculate features
        df["price_24h_avg"] = df[TARGET_VARIABLE].rolling(window=24, min_periods=1).mean()
        df["price_168h_avg"] = df[TARGET_VARIABLE].rolling(window=168, min_periods=1).mean()
        df["price_24h_std"] = df[TARGET_VARIABLE].rolling(window=24, min_periods=1).std()
        df["hour_avg_price"] = df.groupby(df["HourSE"].dt.hour)[TARGET_VARIABLE].transform("mean")
        df["price_vs_hour_avg"] = df[TARGET_VARIABLE] / df["hour_avg_price"]

        # Save to CSV
        try:
            df.to_csv(csv_file_path, index=False)
            print(f'Successfully added {len(new_data)} missing hourly price records and updated features.')
        except Exception as e:
            print(f"Error saving price data to CSV: {e}")
    else:
        print("Price data is already up-to-date.")


def update_grid_data():
    """Update the grid data using Electricity Maps API with hourly resolution"""
    # Use path from config
    grid_file_path = DATA_DIR / SWEDEN_GRID_FILE.name
    
    # Load API key from env file
    dotenv_path = project_root / 'api.env'
    load_dotenv(dotenv_path=dotenv_path)
    api_key = os.getenv('ELECTRICITYMAPS')
    
    if not api_key:
        print("Error: ELECTRICITYMAPS API key not found in api.env.")
        return
    
    # Use grid columns from feature groups config
    grid_cols = FEATURE_GROUPS["grid_cols"]
    
    # Define the essential zones for import/export
    zones = {
        'SE-SE2': 'Main connection from northern Sweden',
        'SE-SE4': 'Main connection to southern Sweden',
        'NO-NO1': 'Norway connection',
        'DK-DK1': 'Denmark connection',
        'FI': 'Finland connection',
    }
    
    # Define the power sources matching config
    power_sources = ['nuclear', 'wind', 'hydro', 'solar', 'unknown']
    
    # Make API request for history data - limit to the last 7 days
    try:
        # Calculate start date (7 days ago)
        start_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%dT%H:%M:%SZ")
        
        response = requests.get(
            f"https://api.electricitymap.org/v3/power-breakdown/history?zone=SE-SE3&start={start_date}",
            headers={
                "auth-token": api_key
            }
        )
        
        if response.status_code != 200:
            print(f"Failed to fetch grid data. Status code: {response.status_code}")
            print("Response content:", response.content.decode() if hasattr(response, 'content') else "No response content")
            return
            
        data = response.json()
        
        # Process records at hourly resolution
        hourly_records = []
        
        for entry in data['history']:
            # Convert datetime to proper format with explicit UTC handling
            try:
                # Parse the timestamp without specifying format (pandas will auto-detect)
                entry_time = pd.to_datetime(entry['datetime'])
                
                # If the timestamp has timezone info, convert to UTC then make naive
                if entry_time.tzinfo is not None:
                    # Convert to UTC
                    entry_time = entry_time.tz_convert('UTC')
                    # Remove timezone info to make it naive
                    entry_time = entry_time.tz_localize(None)
            except Exception as tz_err:
                print(f"Error processing timestamp {entry['datetime']}: {tz_err}")
                # Fallback: try string manipulation if parsing fails
                try:
                    # If it's a string with timezone info like "2025-04-15 19:00:00+00:00"
                    ts_str = str(entry['datetime'])
                    if '+' in ts_str:
                        # Remove the timezone part
                        base_ts = ts_str.split('+')[0]
                        entry_time = pd.to_datetime(base_ts)
                    else:
                        entry_time = pd.to_datetime(ts_str)
                except Exception as parse_err:
                    print(f"Severe error parsing timestamp: {parse_err}. Using current time as fallback.")
                    entry_time = pd.Timestamp.now()
            
            # Create record for this hour
            record = {col: 0 for col in grid_cols}  # Initialize all columns from config
            record['datetime'] = entry_time
            
            # Set core metrics and round to 2 decimals
            record['fossilFreePercentage'] = round(entry.get('fossilFreePercentage', 0) or 0, 2)
            record['renewablePercentage'] = round(entry.get('renewablePercentage', 0) or 0, 2)
            record['powerConsumptionTotal'] = round(entry.get('powerConsumptionTotal', 0) or 0, 2)
            record['powerProductionTotal'] = round(entry.get('powerProductionTotal', 0) or 0, 2)
            record['powerImportTotal'] = round(entry.get('powerImportTotal', 0) or 0, 2)
            record['powerExportTotal'] = round(entry.get('powerExportTotal', 0) or 0, 2)
            
            # Get production breakdown
            prod = entry.get('powerProductionBreakdown', {})
            
            # Set power sources and round
            for source in power_sources:
                record[source] = round(prod.get(source, 0) or 0, 2)
            
            # Set import/export for each zone and round
            import_breakdown = entry.get('powerImportBreakdown', {})
            export_breakdown = entry.get('powerExportBreakdown', {})
            
            for zone in zones:
                record[f'import_{zone}'] = round(import_breakdown.get(zone, 0) or 0, 2)
                record[f'export_{zone}'] = round(export_breakdown.get(zone, 0) or 0, 2)
            
            hourly_records.append(record)
        
        # Create DataFrame with specified columns from config
        df = pd.DataFrame(hourly_records)
        
        # Set datetime as index and ensure it's properly formatted
        if not df.empty:
            df.set_index('datetime', inplace=True)
            
            # Ensure all required columns are present (initialize with zeros if missing)
            for col in grid_cols:
                if col not in df.columns:
                    df[col] = 0
            
            # Select only the columns defined in grid_cols and in the correct order
            df = df[grid_cols]
            df.sort_index(inplace=True)
            
            # Fill any missing values with 0
            df = df.fillna(0)
            
            # Merge with existing data if it exists and is not empty
            if grid_file_path.exists():
                try:
                    existing_df = pd.read_csv(grid_file_path)
                    if not existing_df.empty:
                        # Ensure datetime column exists and convert to proper datetime
                        if 'datetime' in existing_df.columns:
                            existing_df['datetime'] = pd.to_datetime(existing_df['datetime'])
                            existing_df.set_index('datetime', inplace=True)
                        else:
                            # If no datetime column, try using the first column as index
                            existing_df = pd.read_csv(grid_file_path, index_col=0, parse_dates=True)
                        
                        # Handle timezone issues
                        if existing_df.index.tzinfo is not None:
                            existing_df.index = existing_df.index.tz_convert('UTC').tz_localize(None)
                        
                        # Combine data, keeping the newer data for duplicated timestamps
                        combined_df = pd.concat([existing_df, df])
                        combined_df = combined_df[~combined_df.index.duplicated(keep='last')]
                        combined_df.sort_index(inplace=True)
                        
                        # Reset index to save datetime as column
                        combined_df.reset_index(inplace=True)
                        combined_df.rename(columns={'index': 'datetime'}, inplace=True)
                        
                        # Save the updated data
                        combined_df.to_csv(grid_file_path, index=False)
                        print(f'Updated grid data with {len(df)} new records.')
                    else:
                        # If existing file is empty or corrupted, just save the new data
                        df.reset_index(inplace=True)
                        df.to_csv(grid_file_path, index=False)
                        print(f'Created new grid data file with {len(df)} records.')
                except Exception as e:
                    print(f"Error merging with existing grid data: {e}")
                    # If there's an error with the existing file, save the new data as backup
                    backup_path = grid_file_path.with_suffix('.backup.csv')
                    df.reset_index(inplace=True)
                    df.to_csv(backup_path, index=False)
                    print(f'Saved backup grid data to {backup_path}')
            else:
                # No existing file, just save the new data
                df.reset_index(inplace=True)
                df.to_csv(grid_file_path, index=False)
                print(f'Created new grid data file with {len(df)} records.')
        else:
            print("No grid data records received from API.")
    except Exception as e:
        print(f"Error updating grid data: {e}")

def main():
    """Main function to update all data files"""
    # Ensure data directory exists
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    print("Updating price data...")
    update_price_data()
    
    print("\nUpdating grid data...")
    update_grid_data()
    
    print("\nData update complete.")

if __name__ == "__main__":
    main()